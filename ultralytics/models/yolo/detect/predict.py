# Ultralytics YOLO ðŸš€, AGPL-3.0 license

from ultralytics.engine.predictor import BasePredictor
from ultralytics.engine.results import Results, Results_2_5 # HWCHU. ìƒˆë¡œìš´ Results_2_5 import
from ultralytics.utils import ops

# HWCHU. ì´ ì•„ëž˜ importëŠ” DetectionPredictor_2_5ë§Œì„ ìœ„í•œ stream_inferenceë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ í•„ìš”.
from ultralytics.utils import DEFAULT_CFG, LOGGER, MACOS, WINDOWS, callbacks, colorstr, ops # HWCHU
from ultralytics.utils.torch_utils import select_device, smart_inference_mode # HWCHU
import torch # HWCHU
import cv2 # HWCHU
from pathlib import Path # HWCHU

class DetectionPredictor(BasePredictor):
    """
    A class extending the BasePredictor class for prediction based on a detection model.

    Example:
        ```python
        from ultralytics.utils import ASSETS
        from ultralytics.models.yolo.detect import DetectionPredictor

        args = dict(model='yolov8n.pt', source=ASSETS)
        predictor = DetectionPredictor(overrides=args)
        predictor.predict_cli()
        ```
    """

    def postprocess(self, preds, img, orig_imgs):
        """Post-processes predictions and returns a list of Results objects."""
        preds = ops.non_max_suppression(
            preds,
            self.args.conf,
            self.args.iou,
            agnostic=self.args.agnostic_nms,
            max_det=self.args.max_det,
            classes=self.args.classes,
        )

        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)

        results = []
        for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0]):
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
            results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))
        return results


'''HWCHU DetectionPredictor_2_5'''
class DetectionPredictor_2_5(BasePredictor):
    """
    A class extending the BasePredictor class for prediction based on a detection model.

    Example:
        ```python
        from ultralytics.utils import ASSETS
        from ultralytics.models.yolo.detect import DetectionPredictor

        args = dict(model='yolov8n.pt', source=ASSETS)
        predictor = DetectionPredictor(overrides=args)
        predictor.predict_cli()
        ```
    """

    def postprocess(self, preds, img, orig_imgs): # ì—¬ê¸°ì— ë“¤ì–´ì˜¤ëŠ” predsëŠ” (y, x, y_dist, x_dist) ì´ë‹¤.
        """Post-processes predictions and returns a list of Results objects."""
        # HWCHU. non_max_suppression_2_5 ì ìš©ìœ¼ë¡œ 2.5Dì— ë§žëŠ” predsì— ë§žê²Œ nmsë„ ìˆ˜í–‰í•˜ìž.
        preds, preds_dists = ops.non_max_suppression_2_5( # HWCHU. preds_distsì—ëŠ” n_m_s_2_5ì—ì„œ output_distê°€ ë“¤ì–´ê°„ë‹¤.
            preds,
            self.args.conf,
            self.args.iou,
            agnostic=self.args.agnostic_nms,
            max_det=self.args.max_det,
            classes=self.args.classes,
        ) # HWCHU. ì˜ˆ) preds[0].shape (5, 6), preds_dist[0].shape (5, 1)

        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)
        
        # results = []
        results_2_5 = [] # HWCHU. Results_2_5 ê°ì²´ë“¤ì„ ë‹´ì€ ê²ƒìž„ì„ í™•ì‹¤ížˆ í•˜ê¸° ìœ„í•œ results_2_5
        # for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0]):
        for pred, orig_img, img_path, preds_dist in zip(preds, orig_imgs, self.batch[0], preds_dists): # HWCHU. zipë‚´ì— preds_distsë„ ë„£ì–´ì„œ forë¬¸ì— ëŒê²Œ í•¨
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
            # results.append(Results(orig_img, path=img_path, names=self.model.names, boxes=pred))
            results_2_5.append(Results_2_5(orig_img, path=img_path, names=self.model.names, boxes=pred, dists=preds_dist)) # HWCHU. Results_2_5 ìƒì„±(initì‹œ dists argë„ ë„£ì–´ì¤Œ)
        # return results
        return results_2_5 # HWCHU. Results_2_5ë“¤ì„ ë‹´ì€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    

    @smart_inference_mode()
    def stream_inference(self, source=None, model=None, *args, **kwargs):
        """Streams real-time inference on camera feed and saves results to file."""
        if self.args.verbose:
            LOGGER.info("")

        # Setup model
        if not self.model:
            self.setup_model(model)

        with self._lock:  # for thread-safe inference
            # Setup source every time predict is called
            self.setup_source(source if source is not None else self.args.source)

            # Check if save_dir/ label file exists
            if self.args.save or self.args.save_txt:
                (self.save_dir / "labels" if self.args.save_txt else self.save_dir).mkdir(parents=True, exist_ok=True)

            # Warmup model # HWCHU. warmup í•˜ë©´ì„œ í•œ ë²ˆ ëª¨ë¸ í†µê³¼í•œë‹¤.
            if not self.done_warmup:
                self.model.warmup(imgsz=(1 if self.model.pt or self.model.triton else self.dataset.bs, 3, *self.imgsz))
                self.done_warmup = True

            self.seen, self.windows, self.batch = 0, [], None
            profilers = (
                ops.Profile(device=self.device),
                ops.Profile(device=self.device),
                ops.Profile(device=self.device),
            )
            self.run_callbacks("on_predict_start")
            for self.batch in self.dataset:
                self.run_callbacks("on_predict_batch_start")
                paths, im0s, s = self.batch

                # Preprocess
                with profilers[0]:
                    im = self.preprocess(im0s)

                # Inference
                with profilers[1]:
                    preds = self.inference(im, *args, **kwargs)
                    # HWCHU. predsëŠ” Detect_2_5ì˜ forwardê°€ ë±‰ì€ (y, x, y_dist, x_dist)ìž„. training ì•„ë‹ˆê³  predict í•˜ê³  ìžˆìœ¼ë¯€ë¡œ.
                    if self.args.embed:
                        yield from [preds] if isinstance(preds, torch.Tensor) else preds  # yield embedding tensors
                        continue

                # Postprocess
                with profilers[2]:
                    self.results = self.postprocess(preds, im, im0s)
                self.run_callbacks("on_predict_postprocess_end")

                # Visualize, save, write results
                n = len(im0s)
                for i in range(n):
                    self.seen += 1
                    self.results[i].speed = {
                        "preprocess": profilers[0].dt * 1e3 / n,
                        "inference": profilers[1].dt * 1e3 / n,
                        "postprocess": profilers[2].dt * 1e3 / n,
                    }
                    if self.args.verbose or self.args.save or self.args.save_txt or self.args.show:
                        s[i] += self.write_results(i, Path(paths[i]), im, s)

                # Print batch results
                if self.args.verbose:
                    LOGGER.info("\n".join(s))

                self.run_callbacks("on_predict_batch_end")
                yield from self.results

        # Release assets
        for v in self.vid_writer.values():
            if isinstance(v, cv2.VideoWriter):
                v.release()

        # Print final results
        if self.args.verbose and self.seen:
            t = tuple(x.t / self.seen * 1e3 for x in profilers)  # speeds per image
            LOGGER.info(
                f"Speed: %.1fms preprocess, %.1fms inference, %.1fms postprocess per image at shape "
                f"{(min(self.args.batch, self.seen), 3, *im.shape[2:])}" % t
            )
        if self.args.save or self.args.save_txt or self.args.save_crop:
            nl = len(list(self.save_dir.glob("labels/*.txt")))  # number of labels
            s = f"\n{nl} label{'s' * (nl > 1)} saved to {self.save_dir / 'labels'}" if self.args.save_txt else ""
            LOGGER.info(f"Results saved to {colorstr('bold', self.save_dir)}{s}")
        self.run_callbacks("on_predict_end")